import streamlit as st
from streamlit.components.v1 import html
import pickle
import os 
import seaborn as sns
import joblib
import warnings
from function import*
import pandas as pd

os.getcwd()

import nltk
import os

# T·∫°o th∆∞ m·ª•c t√πy ch·ªânh cho d·ªØ li·ªáu NLTK
nltk_data_dir = os.path.join(os.getcwd(), "nltk_data")
os.makedirs(nltk_data_dir, exist_ok=True)

# Th√™m th∆∞ m·ª•c v√†o ƒë∆∞·ªùng d·∫´n t√¨m ki·∫øm c·ªßa NLTK
nltk.data.path.append(nltk_data_dir)

# T·∫£i t√†i nguy√™n 'punkt' v√†o th∆∞ m·ª•c t√πy ch·ªânh
nltk.download('punkt', download_dir=nltk_data_dir)



###################################
import subprocess
import sys
import warnings
warnings.filterwarnings('ignore') # B·ªè qua t·∫•t c·∫£ c·∫£nh b√°o
warnings.filterwarnings("ignore", category=UserWarning)

import random
import pandas as pd
import json
import regex
from underthesea import word_tokenize, pos_tag, sent_tokenize
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import ExtraTreesClassifier
from pyvi import ViTokenizer
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import numpy as np
import requests
import re
import regex
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize, word_tokenize
from pyvi.ViTokenizer import tokenize




#Load pre-trained models
@st.cache_resource
# def load_models():
#     try:
#         # Load t·∫•t c·∫£ models t·ª´ file ƒë√£ train
#         vectorizer = joblib.load('tfidf_vectorizer_new_v1.pkl')
#         model = joblib.load('ExtraTreesClassifier_model_new.pkl')
        
#         # Load label encoder
#         with open('label_encoder_new.pkl', 'rb') as f:
#             loaded_le = pickle.load(f)
#             # print("Loaded classes:", loaded_le.classes_)
            
#         return vectorizer, model, loaded_le
        
#     except Exception as e:
#         st.error(f"L·ªói khi t·∫£i models: {e}")
#         return None, None, None


def load_models():
    try:
        # Load label encoder
        with open('label_encoder_new.pkl', 'rb') as f:
            loaded_le = pickle.load(f)
        
        # Map classes theo ƒë√∫ng th·ª© t·ª± c·ªßa model
        loaded_le.classes_ = np.array(['negative', 'neutral', 'positive'])
        
        # Load vectorizer v√† fit
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),
            min_df=0.02,
            max_df=0.85,
            sublinear_tf=True
        )
        
        df = pd.read_csv('df_clean_v1.csv')
        df['new_content'] = df['new_content'].fillna('')
        vectorizer.fit(df['new_content'])
        
        # Load model
        model = joblib.load('ExtraTreesClassifier_model_new.pkl')
        
        print("Model expects:", model.n_features_in_)
        print("Vectorizer vocabulary size:", len(vectorizer.vocabulary_))
        print("Classes mapping:", dict(enumerate(loaded_le.classes_)))
        
        return vectorizer, model, loaded_le
        
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i models: {e}")
        return None, None, None




# Load models v√† hi·ªÉn th·ªã labels
vectorizer, model, loaded_le = load_models()
def process_text(text, emoji_dict, teen_dict, wrong_lst):
    document = text.lower()
    document = re.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        # CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
        # # TRANSLATE English -> Vietnamese
        # sentence = ' '.join(english_dict[word] if word in english_dict else word for word in sentence.split())
        # new_sentence = new_sentence+ sentence + '. '

    document = new_sentence

    document = re.sub(r'((http|https)\:\/\/)?[a-zA-Z0-9\.\/\?\:@\-_=#]+\.([a-zA-Z]){2,6}([a-zA-Z0-9\.\&\/\?\:@\-_=#])*', '',document)
    document = re.sub(r'[\r\n]+', ' ', document)
    document = re.sub('[^\w\s]', ' ', document)
    document = re.sub('[\s]{2,}', ' ', document)
    document = re.sub('^[\s]{1,}', '', document)
    document = re.sub('[\s]{1,}$', '', document)
    punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~‚Äô'
    for char in punctuation:
        document = document.replace(char, ' ')

    return document

# Normalize unicode Vietnamese
def loaddicchar():
    uniChars = "√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'.split(
        '|')
    charutf8 = "√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# Unicode Vietnamese
def covert_unicode(txt):
    dicchar = loaddicchar()
    return re.sub(
        r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£',
        lambda x: dicchar[x.group()], txt)

def find_words(document, list_of_words):
    document_lower = document.lower()
    word_count = 0
    word_list = []

    for word in list_of_words:
        if word in document_lower:
            word_count += document_lower.count(word)
            word_list.append(word)

    return word_count, word_list


def remove_stopword(text, stopwords):
    """
    Lo·∫°i b·ªè stopwords kh·ªèi vƒÉn b·∫£n.
    """
    # Ki·ªÉm tra n·∫øu text l√† danh s√°ch th√¨ n·ªëi th√†nh chu·ªói
    if isinstance(text, list):
        text = ' '.join(text)

    # Lo·∫°i b·ªè stopwords
    document = ' '.join('' if word in stopwords else word for word in text.split())

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    document = regex.sub(r'\s+', ' ', document).strip()
    return document

##LOAD EMOJICON
file = open('files/emojicon.txt', 'r', encoding="utf8")
emoji_lst = file.read().split('\n')
emoji_dict = {}
for line in emoji_lst:
    key, value = line.split('\t')
    emoji_dict[key] = str(value)
file.close()
#################
#LOAD TEENCODE
file = open('files/teencode.txt', 'r', encoding="utf8")
teen_lst = file.read().split('\n')
teen_dict = {}
for line in teen_lst:
    key, value = line.split('\t')
    teen_dict[key] = str(value)
file.close()
###############
#LOAD TRANSLATE ENGLISH -> VNMESE
file = open('files/english-vnmese.txt', 'r', encoding="utf8")
english_lst = file.read().split('\n')
english_dict = {}
for line in english_lst:
    key, value = line.split('\t')
    english_dict[key] = str(value)
file.close()
################
#LOAD wrong words
file = open('files/wrong-word.txt', 'r', encoding="utf8")
wrong_lst = file.read().split('\n')
file.close()
#################
#LOAD STOPWORDS
file = open('files/vietnamese-stopwords.txt', 'r', encoding="utf8")
stopwords_lst = file.read().split('\n')
file.close()

neutral_words = ["ch·∫•p nh·∫≠n ƒë∆∞·ª£c", "trung b√¨nh", "b√¨nh th∆∞·ªùng", "t·∫°m ·ªïn", "trung l·∫≠p", "c√≥ th·ªÉ"
                 "kh√¥ng n·ªïi b·∫≠t", "ƒë·ªß ·ªïn", "ƒë·ªß t·ªët", "c√≥ th·ªÉ ch·∫•p nh·∫≠n", "b√¨nh th∆∞·ªùng",
                 "th∆∞·ªùng xuy√™n", "t∆∞∆°ng ƒë·ªëi", "h·ª£p l√Ω", "t∆∞∆°ng t·ª±",
                 "c√≥ th·ªÉ s·ª≠ d·ª•ng", "b√¨nh y√™n", "b√¨nh tƒ©nh", "kh√¥ng qu√° t·ªá", "trung h·∫°ng",
                 "c√≥ th·ªÉ ƒëi·ªÉm c·ªông", "d·ªÖ ch·∫•p nh·∫≠n", "kh√¥ng ph·∫£i l√† v·∫•n ƒë·ªÅ",
                 "kh√¥ng ph·∫£n ƒë·ªëi", "kh√¥ng qu√° ƒë√°ng k·ªÉ", "kh√¥ng g√¢y b·∫•t ng·ªù", "kh√¥ng t·∫°o ·∫•n t∆∞·ª£ng", "c√≥ th·ªÉ ch·∫•p nh·∫≠n",
                 "kh√¥ng g√¢y s·ªëc", "t∆∞∆°ng ƒë·ªëi t·ªët", "kh√¥ng thay ƒë·ªïi", "kh√¥ng qu√° ph·ª©c t·∫°p", "kh√¥ng ƒë√°ng k·ªÉ",
                 "ch·∫•p nh·∫≠n", "c√≥ th·ªÉ d·ªÖ d√†ng th√≠ch nghi", "kh√¥ng qu√° c·∫ßu k·ª≥", "kh√¥ng c·∫ßn thi·∫øt", "kh√¥ng y√™u c·∫ßu nhi·ªÅu", "kh√¥ng g√¢y h·∫°i",
                 "kh√¥ng c√≥ s·ª± thay ƒë·ªïi ƒë√°ng k·ªÉ", "kh√¥ng r√µ r√†ng", "kh√¥ng qu√° ph√™ b√¨nh", "kh√¥ng ƒë√°ng ch√∫ √Ω", "kh√¥ng ƒë·∫∑c bi·ªát",
                 "kh√¥ng qu√° ph·ª©c t·∫°p", "kh√¥ng g√¢y phi·ªÅn h√†", "kh√¥ng ƒë√°ng k·ªÉ", "kh√¥ng g√¢y k√≠ch th√≠ch"]

negative_words = [
    "k√©m", "t·ªá", "ƒëau", "x·∫•u", "b·ªã","r√®", "·ªìn",
    "bu·ªìn", "r·ªëi", "th√¥", "l√¢u", "sai", "h∆∞", "d∆°", "kh√¥ng c√≥"
    "t·ªëi", "ch√°n", "√≠t", "m·ªù", "m·ªèng", "v·ª°", "h∆∞ h·ªèng",
    "l·ªèng l·∫ªo", "kh√≥", "c√πi", "y·∫øu", "m√†", "kh√¥ng th√≠ch", "kh√¥ng th√∫ v·ªã", "kh√¥ng ·ªïn",
    "kh√¥ng h·ª£p", "kh√¥ng ƒë√°ng tin c·∫≠y", "kh√¥ng chuy√™n nghi·ªáp", "nh·∫ßm l·∫´n"
    "kh√¥ng ph·∫£n h·ªìi", "kh√¥ng an to√†n", "kh√¥ng ph√π h·ª£p", "kh√¥ng th√¢n thi·ªán", "kh√¥ng linh ho·∫°t", "kh√¥ng ƒë√°ng gi√°",
    "kh√¥ng ·∫•n t∆∞·ª£ng", "kh√¥ng t·ªët", "ch·∫≠m", "kh√≥ khƒÉn", "ph·ª©c t·∫°p", "b·ªã m·ªü", "b·ªã khui", "kh√¥ng ƒë√∫ng", "kh√¥ng ƒë√∫ng s·∫£n ph·∫©m",
    "kh√≥ hi·ªÉu", "kh√≥ ch·ªãu", "g√¢y kh√≥ d·ªÖ", "r∆∞·ªùm r√†", "kh√≥ truy c·∫≠p", "b·ªã b√≥c", "sai s·∫£n ph·∫©m",
    "th·∫•t b·∫°i", "t·ªìi t·ªá", "kh√≥ x·ª≠", "kh√¥ng th·ªÉ ch·∫•p nh·∫≠n", "t·ªìi t·ªá","kh√¥ng r√µ r√†ng", "gi·∫£m ch·∫•t l∆∞·ª£ng",
    "kh√¥ng ch·∫Øc ch·∫Øn", "r·ªëi r·∫Øm", "kh√¥ng ti·ªán l·ª£i", "kh√¥ng ƒë√°ng ti·ªÅn", "ch∆∞a ƒë·∫πp", "kh√¥ng ƒë·∫πp"
]

positive_words = [
    "th√≠ch", "t·ªët", "xu·∫•t s·∫Øc","ƒë√∫ng", "tuy·ªát v·ªùi", "tuy·ªát h·∫£o", "ƒë·∫πp", "·ªïn"
    "h√†i l√≤ng", "∆∞ng √Ω", "ho√†n h·∫£o", "ch·∫•t l∆∞·ª£ng", "th√∫ v·ªã", "nhanh"
    "ti·ªán l·ª£i", "d·ªÖ s·ª≠ d·ª•ng", "hi·ªáu qu·∫£", "·∫•n t∆∞·ª£ng",
    "n·ªïi b·∫≠t", "t·∫≠n h∆∞·ªüng", "t·ªën √≠t th·ªùi gian", "th√¢n thi·ªán", "h·∫•p d·∫´n",
    "g·ª£i c·∫£m", "t∆∞∆°i m·ªõi", "l·∫° m·∫Øt", "cao c·∫•p", "ƒë·ªôc ƒë√°o",
    "h·ª£p kh·∫©u v·ªã", "r·∫•t t·ªët", "r·∫•t th√≠ch", "t·∫≠n t√¢m", "ƒë√°ng tin c·∫≠y", "ƒë·∫≥ng c·∫•p",
    "h·∫•p d·∫´n", "an t√¢m", "kh√¥ng th·ªÉ c∆∞·ª°ng_l·∫°i", "th·ªèa m√£n", "th√∫c ƒë·∫©y",
    "c·∫£m ƒë·ªông", "ph·ª•c v·ª• t·ªët", "l√†m h√†i l√≤ng", "g√¢y ·∫•n t∆∞·ª£ng", "n·ªïi tr·ªôi",
    "s√°ng t·∫°o", "qu√Ω b√°u", "ph√π h·ª£p", "t·∫≠n t√¢m",
    "hi·∫øm c√≥", "c·∫£i thi·ªán", "ho√† nh√£", "chƒÉm ch·ªâ", "c·∫©n th·∫≠n",
    "vui v·∫ª", "s√°ng s·ªßa", "h√†o h·ª©ng", "ƒëam m√™", "v·ª´a v·∫∑n", "ƒë√°ng ti·ªÅn"
]

# List of words with negative meanings
negation_words = ["kh√¥ng", "nh∆∞ng", "tuy nhi√™n", "m·∫∑c d√π", "ch·∫≥ng", "m√†", 'k√©m', 'gi·∫£m']

positive_emojis = [
    "üòÑ", "üòÉ", "üòÄ", "üòÅ", "üòÜ",
    "üòÖ", "ü§£", "üòÇ", "üôÇ", "üôÉ",
    "üòâ", "üòä", "üòá", "ü•∞", "üòç",
    "ü§©", "üòò", "üòó", "üòö", "üòô",
    "üòã", "üòõ", "üòú", "ü§™", "üòù",
    "ü§ó", "ü§≠", "ü•≥", "üòå", "üòé",
    "ü§ì", "üßê", "üëç", "ü§ù", "üôå", "üëè", "üëã",
    "ü§ô", "‚úã", "üñêÔ∏è", "üëå", "ü§û",
    "‚úåÔ∏è", "ü§ü", "üëà", "üëâ", "üëÜ",
    "üëá", "‚òùÔ∏è"
]

# Count emojis positive and negative
negative_emojis = [
    "üòû", "üòî", "üôÅ", "‚òπÔ∏è", "üòï",
    "üò¢", "üò≠", "üòñ", "üò£", "üò©",
    "üò†", "üò°", "ü§¨", "üò§", "üò∞",
    "üò®", "üò±", "üò™", "üòì", "ü•∫",
    "üòí", "üôÑ", "üòë", "üò¨", "üò∂",
    "ü§Ø", "üò≥", "ü§¢", "ü§Æ", "ü§ï",
    "ü•¥", "ü§î", "üò∑", "üôÖ‚Äç‚ôÇÔ∏è", "üôÖ‚Äç‚ôÄÔ∏è",
    "üôÜ‚Äç‚ôÇÔ∏è", "üôÜ‚Äç‚ôÄÔ∏è", "üôá‚Äç‚ôÇÔ∏è", "üôá‚Äç‚ôÄÔ∏è", "ü§¶‚Äç‚ôÇÔ∏è",
    "ü§¶‚Äç‚ôÄÔ∏è", "ü§∑‚Äç‚ôÇÔ∏è", "ü§∑‚Äç‚ôÄÔ∏è", "ü§¢", "ü§ß",
    "ü§®", "ü§´", "üëé", "üëä", "‚úä", "ü§õ", "ü§ú",
    "ü§ö", "üñï"
]



# Load external files
def load_resources():
    files = {
        "emoji_dict": 'files/emojicon.txt',
        "teen_dict": 'files/teencode.txt',
        "wrong_lst": 'files/wrong-word.txt',
        "stopwords_lst": 'files/vietnamese-stopwords.txt',
    }
    resources = {}
    for key, path in files.items():
        with open(path, 'r', encoding='utf8') as file:
            if key.endswith('_dict'):
                resources[key] = {line.split('\t')[0]: line.split('\t')[1] for line in file.read().split('\n')}
            else:
                resources[key] = file.read().split('\n')
    return resources

resources = load_resources()

def predict_sentiment(text, vectorizer, model, loaded_le):
    try:
        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
        processed_text = process_text(text, emoji_dict, teen_dict, wrong_lst)
        processed_text = covert_unicode(processed_text)
        processed_text = remove_stopword(processed_text, stopwords_lst)
        
        # Vector h√≥a text
        text_features = vectorizer.transform([processed_text])
        
        # ƒê·∫øm t·ª´ theo lo·∫°i
        neutral_count = find_words(processed_text, neutral_words)[0]
        negative_count = find_words(processed_text, negative_words)[0]
        positive_count = find_words(processed_text, positive_words)[0]
        positive_emoji_count = find_words(text, positive_emojis)[0]
        negative_emoji_count = find_words(text, negative_emojis)[0]
        
        # K·∫øt h·ª£p features
        additional_features = np.array([
            neutral_count,
            negative_count,
            positive_count,
            positive_emoji_count,
            negative_emoji_count
        ]).reshape(1, -1)
        
        combined_features = hstack((text_features, additional_features))
        
        # D·ª± ƒëo√°n x√°c su·∫•t cho t·ª´ng class
        probas = model.predict_proba(combined_features)[0]
        print("Probabilities:", dict(zip(loaded_le.classes_, probas)))
        
        # X√°c ƒë·ªãnh sentiment d·ª±a tr√™n rules
        if negative_count > positive_count:
            sentiment = 'negative'
        elif positive_count > negative_count:
            sentiment = 'positive'
        elif neutral_count > 0:
            sentiment = 'neutral'
        else:
            # N·∫øu kh√¥ng c√≥ t·ª´ ƒë·∫∑c tr∆∞ng n√†o, d√πng k·∫øt qu·∫£ t·ª´ model
            prediction = model.predict(combined_features)[0]
            sentiment = loaded_le.classes_[prediction]
        
        print("Word counts:", {
            'neutral': neutral_count,
            'negative': negative_count,
            'positive': positive_count
        })
        print("Final sentiment:", sentiment)
        
        return {
            'sentiment': sentiment,
            'processed_text': processed_text,
            'features': {
                'neutral_words': neutral_count,
                'negative_words': negative_count,
                'positive_words': positive_count,
                'positive_emojis': positive_emoji_count,
                'negative_emojis': negative_emoji_count
            }
        }
    except Exception as e:
        print(f"Error details: {e}")
        return {'error': str(e)}






######################################
try:
    df_summary = pd.read_csv('df_summary.csv')
except Exception as e:
    st.error(f"Error loading data: {e}")

df_goc = df_summary[['id','ma_khach_hang', 'noi_dung_binh_luan', 'ngay_binh_luan', 
                    'gio_binh_luan', 'so_sao', 'ma_san_pham']]

# Sidebar menu
menu = ["Home","Bussiness Problem","Data Preprocessing","EDA","Modeling - Evaluation", "Comment Sentiment Analysis", "Product Sentiment Analysis"]

choice = st.sidebar.selectbox("Menu", menu)

if choice == "Home":
    # Title and logo
    logo_path = "image/logoTTTH.png"  # Update path if necessary
    st.image(logo_path, width=150)

    # Title with gradient effect
    st.markdown("""
    <h1 style="text-align: center; color: #ff6347; font-size: 50px; background: linear-gradient(to right, #ff7f50, #ff6347); -webkit-background-clip: text; color: transparent;">
        Trung T√¢m Tin H·ªçc
    </h1>
    <h3 style="text-align: center; color: #4682B4;">
        Tr∆∞·ªùng ƒêH Khoa H·ªçc T·ª± Nhi√™n Tp. H·ªì Ch√≠ Minh
    </h3>
    """, unsafe_allow_html=True)

    # Subtitle
    st.markdown("""
    <h2 style="color: #32CD32; text-align: center;">
        ƒê·ªì √°n t·ªët nghi·ªáp Data Science
    </h2>
    <h3 style="text-align: center; font-style: italic; color: #4169E1;">
        Project 01: <b>Sentiment Analysis</b>
    </h3>
    """, unsafe_allow_html=True)

    # Display authors with styled text
    st.markdown("""
    <p style="text-align: center; color: #6A5ACD; font-size: 18px;">
        <b>Phan Minh Hu·ªá</b><br>
        <b>Hu·ª≥nh Danh Nh√¢n</b>
    </p>
    """, unsafe_allow_html=True)

    # Content outline with animated text color
    st.markdown("""
    <h4 style="color: #8B0000;">**N·ªôi dung:**</h4>
    <ul style="font-size: 18px; line-height: 1.8; color: #483D8B;">
        <li><b>Business Understanding</b></li>
        <li><b>Data Preprocessing</b></li>
        <li><b>Exploratory Data Analysis (EDA)</b></li>
        <li><b>Modeling - Evaluation</b></li>
        <li><b>Comment Sentiment Analysis</b></li>
        <li><b>Product Sentiment Analysis</b></li>
    </ul>
    """, unsafe_allow_html=True)

    # Add a background effect for the page
    background_css = """
    <style>
    body {
        background: linear-gradient(to bottom, #f0f8ff, #ffffff);
    }
    </style>
    """
    st.markdown(background_css, unsafe_allow_html=True)





elif choice == "Bussiness Problem":
    # Add a custom background using HTML and CSS
    st.markdown(
        """
        <style>
        body {
            background-color: #f0f2f6;
            color: #333333;
        }
        .main {
            background-color: #ffffff;
            border-radius: 10px;
            padding: 20px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
        }
        h1 {
            color: #4b7bec;
            text-align: center;
            font-size: 32px;
        }
        h2 {
            color: #3742fa;
            font-size: 24px;
        }
        ul {
            font-size: 18px;
            color: #2f3542;
        }
        a {
            color: #1e90ff;
            font-weight: bold;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # Add an image banner with a custom width
    image_path = "image/hasaki_banner_2.jpg"  # Update the path if necessary
    st.image(image_path, use_container_width =True, caption="Welcome to HASAKI")

    # Add a hyperlink in bold
    st.markdown(
        "**üåê T√¨m hi·ªÉu th√™m t·∫°i:** [Hasaki.vn](https://hasaki.vn)",
        unsafe_allow_html=True,
    )
    
    # Title with styled font
    st.markdown('<h1>Bussiness Problem</h1>', unsafe_allow_html=True)

    # Introduction section
    st.markdown('<h2>Gi·ªõi thi·ªáu</h2>', unsafe_allow_html=True)
    st.markdown(
        """
        <ul>
            <li><b>V·∫•n ƒë·ªÅ Kinh Doanh:</b> HASAKI.VN, h·ªá th·ªëng c·ª≠a h√†ng m·ªπ ph·∫©m v√† d·ªãch v·ª• chƒÉm s√≥c s·∫Øc ƒë·∫πp h√†ng ƒë·∫ßu, c·∫ßn ph√¢n t√≠ch c√°c ƒë√°nh gi√° t·ª´ kh√°ch h√†ng ƒë·ªÉ:</li>
        </ul>
        """,
        unsafe_allow_html=True
    )
    st.markdown(
        """
        <ul>
            <li>Hi·ªÉu r√µ h∆°n v·ªÅ √Ω ki·∫øn v√† nhu c·∫ßu c·ªßa kh√°ch h√†ng.</li>
            <li>N·∫Øm b·∫Øt ph·∫£n h·ªìi v·ªÅ s·∫£n ph·∫©m v√† d·ªãch v·ª•.</li>
            <li>C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m c≈©ng nh∆∞ c√°c d·ªãch v·ª• ƒëi k√®m, t·ª´ ƒë√≥ n√¢ng cao s·ª± h√†i l√≤ng v√† l√≤ng trung th√†nh c·ªßa kh√°ch h√†ng.</li>
        </ul>
        """,
        unsafe_allow_html=True,
    )

elif choice == "Data Preprocessing":
    # Title
    st.title("Data Preprocessing")
    
    # Introduction
    st.write("### 1. Data Overview")
    st.write("""
    B·ªô d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p g·ªìm c√°c t·ªáp:
    - **Danh_gia.csv**
    - **San_pham.csv**
    - **Khach_hang.csv**
    
    S·ª≠ d·ª•ng file **Danh_gia.csv** ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh ƒë√°nh gi√°.
    """)
    # Select the required columns


    # Display the table
    st.write("### D·ªØ li·ªáu m·∫´u")
    st.dataframe(df_goc.head(5))  # Display first 5 rows
    # Data preprocessing steps
    st.write("### 2. Data Preprocessing Steps")
    st.markdown("""
    C√°c b∆∞·ªõc x·ª≠ l√≠:
    - üìù **Convert c√°c t·ª´ ti·∫øng Anh**  
    - üîß **X·ª≠ l√≠ wrong word**  
    - ‚ùå **X·ª≠ l√≠ stop word**  
    - üé® **X·ª≠ l√≠ c√°c icon**  
    - üëç **Ph√¢n lo·∫°i positive word**  
    - üëé **Ph√¢n lo·∫°i negative word**  
    - üòê **Ph√¢n lo·∫°i neutral word**  
    - ...  
    """)

    # Show another sample from df_summary
    st.write("### D·ªØ li·ªáu sau khi ƒë√£ ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu")
    st.dataframe(df_summary.head(5))

elif choice == "EDA":
    # Title
    st.title("Exploratory Data Analysis (EDA)")
    
    # Overview of the dataset
    st.write("### T·ªïng quan")
    st.write("""
    B·ªô d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p:
    - **7 c·ªôt v√† 21575 d√≤ng**
    - **C·ªôt `noi_dung_binh_luan` c√≥ 901 d√≤ng b·ªã null**
    """)
    # Subsection: Rating Distribution
    st.write("### Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo s·ªë sao")
    
    # Calculate the counts for each rating level
    rating_counts = df_goc.groupby('so_sao')['so_sao'].count()
    
    # Create the bar chart
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(rating_counts.index, rating_counts.values, color='skyblue', edgecolor='black')
    ax.set_xlabel('S·ªë sao')
    ax.set_ylabel('S·ªë l∆∞·ª£ng ƒë√°nh gi√°')
    ax.set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo s·ªë sao')
    
    # Add value annotations on top of each bar
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height, f'{int(height):,}', ha='center', va='bottom', fontsize=10)
    
    # Adjust x-axis ticks to show integer values only
    ax.set_xticks(rating_counts.index)

    # Show the chart in Streamlit
    st.pyplot(fig)

    # Display detailed statistics
    st.write("### Th·ªëng k√™ chi ti·∫øt s·ªë l∆∞·ª£ng ƒë√°nh gi√°")
    stats_df = pd.DataFrame({
        'S·ªë l∆∞·ª£ng ƒë√°nh gi√°': rating_counts,
        'T·ª∑ l·ªá %': (rating_counts / rating_counts.sum() * 100).round(2)
    })
    st.dataframe(stats_df)

    # Additional statistics
    st.write(f"**T·ªïng s·ªë ƒë√°nh gi√°:** {rating_counts.sum():,}")
    st.write(f"**ƒêi·ªÉm trung b√¨nh:** {(rating_counts * rating_counts.index).sum() / rating_counts.sum():.2f}")
    st.write(f"**M·ª©c ƒë√°nh gi√° ph·ªï bi·∫øn nh·∫•t:** {rating_counts.idxmax()} sao ({rating_counts.max():,} ƒë√°nh gi√°)")

    # Insights
    st.markdown("""
    ### Nh·∫≠n x√©t:
    - Bi·ªÉu ƒë·ªì cho th·∫•y s·ª± h√†i l√≤ng r·∫•t cao c·ªßa kh√°ch h√†ng v·ªõi s·∫£n ph·∫©m/d·ªãch v·ª•, th·ªÉ hi·ªán qua t·ª∑ l·ªá ƒë√°nh gi√° 5 sao v∆∞·ª£t tr·ªôi.
    - Tuy nhi√™n, t·ª∑ l·ªá ƒë√°nh gi√° t·ª´ 1-5 sao b·ªã ch√™nh l·ªách r·∫•t l·ªõn => d·ªØ li·ªáu m·∫•t c√¢n b·∫±ng.
    """)
    # Subsection: Customer Classification
    st.write("### Ph√¢n lo·∫°i kh√°ch h√†ng theo s·ªë l∆∞·ª£ng ƒë√°nh gi√°")

    # Define the categorization function
    def categorize_customer(n_reviews):
        if n_reviews == 1:
            return 'M·ªôt l·∫ßn'
        elif n_reviews <= 3:
            return '√çt (2-3)'
        elif n_reviews <= 5:
            return 'Trung b√¨nh (4-5)'
        else:
            return 'Nhi·ªÅu (>5)'

    # Apply categorization
    customer_reviews = df_goc.groupby('ma_khach_hang').size().reset_index()
    customer_reviews.columns = ['ma_khach_hang', 'so_luong_danh_gia']
    customer_reviews['nhom_khach_hang'] = customer_reviews['so_luong_danh_gia'].apply(categorize_customer)

    # Plot the customer classification chart
    st.write("#### Bi·ªÉu ƒë·ªì ph√¢n lo·∫°i kh√°ch h√†ng")
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.countplot(
        data=customer_reviews,
        x='nhom_khach_hang',
        order=['M·ªôt l·∫ßn', '√çt (2-3)', 'Trung b√¨nh (4-5)', 'Nhi·ªÅu (>5)'],
        palette="coolwarm",
        ax=ax
    )
    ax.set_title('Ph√¢n lo·∫°i kh√°ch h√†ng theo s·ªë l∆∞·ª£ng ƒë√°nh gi√°')
    ax.set_xlabel('Nh√≥m kh√°ch h√†ng')
    ax.set_ylabel('S·ªë l∆∞·ª£ng kh√°ch h√†ng')
    ax.tick_params(axis='x', rotation=45)

    # Display the chart
    st.pyplot(fig)

    # Observations
    st.markdown("""
    ### Nh·∫≠n x√©t:
    - Kh√°ch h√†ng ƒë√°nh gi√° 2-3 l·∫ßn v√† >5 l·∫ßn cao => kh√°ch h√†ng trung th√†nh.
    - Kh√°ch h√†ng ch·ªâ ƒë√°nh gi√° 1 l·∫ßn: c·∫ßn t·∫≠p trung ph√¢n t√≠ch.
    """)
    # Subsection: Distribution of Customer Reviews
    st.write("### Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo kh√°ch h√†ng")
    
    # Plot histogram
    fig, ax = plt.subplots(figsize=(12, 6))
    sns.histplot(data=customer_reviews, x='so_luong_danh_gia', bins=30, kde=True, color='blue', ax=ax)
    ax.set_title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo kh√°ch h√†ng', fontsize=14)
    ax.set_xlabel('S·ªë l∆∞·ª£ng ƒë√°nh gi√°', fontsize=12)
    ax.set_ylabel('S·ªë l∆∞·ª£ng kh√°ch h√†ng', fontsize=12)

    # Display the histogram
    st.pyplot(fig)

    # Basic Statistics
    total_customers = len(customer_reviews)
    avg_reviews = customer_reviews['so_luong_danh_gia'].mean()
    max_reviews = customer_reviews['so_luong_danh_gia'].max()
    min_reviews = customer_reviews['so_luong_danh_gia'].min()

    st.write("### Th·ªëng k√™ c∆° b·∫£n v·ªÅ s·ªë l∆∞·ª£ng ƒë√°nh gi√° c·ªßa kh√°ch h√†ng")
    st.write(f"- **T·ªïng s·ªë kh√°ch h√†ng:** {total_customers:,}")
    st.write(f"- **Trung b√¨nh s·ªë ƒë√°nh gi√°/kh√°ch:** {avg_reviews:.2f}")
    st.write(f"- **S·ªë ƒë√°nh gi√° cao nh·∫•t c·ªßa m·ªôt kh√°ch:** {max_reviews}")
    st.write(f"- **S·ªë ƒë√°nh gi√° th·∫•p nh·∫•t c·ªßa m·ªôt kh√°ch:** {min_reviews}")

    # Insights
    st.markdown("""
    ### Nh·∫≠n x√©t:
    - H·∫ßu h·∫øt kh√°ch h√†ng ch·ªâ ƒë∆∞a ra m·ªôt s·ªë l∆∞·ª£ng ƒë√°nh gi√° nh·ªè.
    - Ph·∫ßn l·ªõn c√°c kh√°ch h√†ng ch·ªâ th·ª±c hi·ªán t·ª´ 1-10 ƒë√°nh gi√°.
    """)

    # 1. Statistics by Year
    yearly_stats = df_summary.groupby('nam').agg({
        'ma_khach_hang': 'count',
        'so_sao': 'mean'
    }).round(2)
    yearly_stats.columns = ['S·ªë l∆∞·ª£ng ƒë√°nh gi√°', 'ƒêi·ªÉm trung b√¨nh']
    yearly_stats = yearly_stats.sort_values('S·ªë l∆∞·ª£ng ƒë√°nh gi√°', ascending=False)

    # 2. Statistics by Month
    monthly_stats = df_summary.groupby('thang').agg({
        'ma_khach_hang': 'count',
        'so_sao': 'mean'
    }).round(2)
    monthly_stats.columns = ['S·ªë l∆∞·ª£ng ƒë√°nh gi√°', 'ƒêi·ªÉm trung b√¨nh']
    monthly_stats = monthly_stats.sort_values('S·ªë l∆∞·ª£ng ƒë√°nh gi√°', ascending=False)

    # 3. Statistics by Weekday
    df_summary['thu'] = pd.to_datetime(df_summary['ngay_binh_luan']).dt.day_name()
    weekday_stats = df_summary.groupby('thu').agg({
        'ma_khach_hang': 'count',
        'so_sao': 'mean'
    }).round(2)
    weekday_stats.columns = ['S·ªë l∆∞·ª£ng ƒë√°nh gi√°', 'ƒêi·ªÉm trung b√¨nh']
    weekday_stats = weekday_stats.sort_values('S·ªë l∆∞·ª£ng ƒë√°nh gi√°', ascending=False)

    # Display statistics
    st.write("#### Th·ªëng k√™ theo nƒÉm:")
    st.dataframe(yearly_stats)

    st.write("#### Th·ªëng k√™ theo th√°ng:")
    st.dataframe(monthly_stats)

    st.write("#### Th·ªëng k√™ theo ng√†y trong tu·∫ßn:")
    st.dataframe(weekday_stats)

    # Visualization
    st.write("### Bi·ªÉu ƒë·ªì th·ªëng k√™")

    # Visualization by Year
    st.write("#### S·ªë l∆∞·ª£ng v√† ƒëi·ªÉm trung b√¨nh ƒë√°nh gi√° theo nƒÉm")
    fig_year, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
    yearly_stats['S·ªë l∆∞·ª£ng ƒë√°nh gi√°'].plot(kind='bar', ax=ax1, color='skyblue')
    ax1.set_title("S·ªë l∆∞·ª£ng ƒë√°nh gi√° theo nƒÉm")
    ax1.set_xlabel("NƒÉm")
    ax1.set_ylabel("S·ªë l∆∞·ª£ng ƒë√°nh gi√°")
    ax1.tick_params(axis='x', rotation=45)

    yearly_stats['ƒêi·ªÉm trung b√¨nh'].plot(kind='bar', ax=ax2, color='orange')
    ax2.set_title("ƒêi·ªÉm trung b√¨nh theo nƒÉm")
    ax2.set_xlabel("NƒÉm")
    ax2.set_ylabel("ƒêi·ªÉm trung b√¨nh")
    ax2.tick_params(axis='x', rotation=45)

    st.pyplot(fig_year)

    # Visualization by Month
    st.write("#### S·ªë l∆∞·ª£ng v√† ƒëi·ªÉm trung b√¨nh ƒë√°nh gi√° theo th√°ng")
    fig_month, (ax3, ax4) = plt.subplots(2, 1, figsize=(10, 10))
    monthly_stats['S·ªë l∆∞·ª£ng ƒë√°nh gi√°'].plot(kind='bar', ax=ax3, color='skyblue')
    ax3.set_title("S·ªë l∆∞·ª£ng ƒë√°nh gi√° theo th√°ng")
    ax3.set_xlabel("Th√°ng")
    ax3.set_ylabel("S·ªë l∆∞·ª£ng ƒë√°nh gi√°")
    ax3.tick_params(axis='x', rotation=45)

    monthly_stats['ƒêi·ªÉm trung b√¨nh'].plot(kind='bar', ax=ax4, color='orange')
    ax4.set_title("ƒêi·ªÉm trung b√¨nh theo th√°ng")
    ax4.set_xlabel("Th√°ng")
    ax4.set_ylabel("ƒêi·ªÉm trung b√¨nh")
    ax4.tick_params(axis='x', rotation=45)

    st.pyplot(fig_month)

    # Visualization by Weekday
    st.write("#### S·ªë l∆∞·ª£ng v√† ƒëi·ªÉm trung b√¨nh ƒë√°nh gi√° theo ng√†y trong tu·∫ßn")
    fig_weekday, ax5 = plt.subplots(figsize=(10, 6))
    weekday_stats['S·ªë l∆∞·ª£ng ƒë√°nh gi√°'].plot(kind='bar', ax=ax5, color='skyblue')
    ax5.set_title("S·ªë l∆∞·ª£ng ƒë√°nh gi√° theo ng√†y trong tu·∫ßn")
    ax5.set_xlabel("Th·ª©")
    ax5.set_ylabel("S·ªë l∆∞·ª£ng ƒë√°nh gi√°")
    ax5.tick_params(axis='x', rotation=45)

    st.pyplot(fig_weekday)

    # Key Insights
    st.write("### Nh·∫≠n x√©t:")
    st.markdown(f"""
    - NƒÉm c√≥ nhi·ªÅu ƒë√°nh gi√° nh·∫•t: **{yearly_stats.index[0]}** ({yearly_stats.iloc[0, 0]:,.0f} ƒë√°nh gi√°).
    - NƒÉm c√≥ ƒëi·ªÉm trung b√¨nh cao nh·∫•t: **{yearly_stats['ƒêi·ªÉm trung b√¨nh'].idxmax()}** ({yearly_stats['ƒêi·ªÉm trung b√¨nh'].max():.2f} sao).
    - Th√°ng c√≥ nhi·ªÅu ƒë√°nh gi√° nh·∫•t: **Th√°ng {monthly_stats.index[0]}** ({monthly_stats.iloc[0, 0]:,.0f} ƒë√°nh gi√°).
    - Th√°ng c√≥ ƒëi·ªÉm trung b√¨nh cao nh·∫•t: **Th√°ng {monthly_stats['ƒêi·ªÉm trung b√¨nh'].idxmax()}** ({monthly_stats['ƒêi·ªÉm trung b√¨nh'].max():.2f} sao).
    - Ng√†y c√≥ nhi·ªÅu ƒë√°nh gi√° nh·∫•t: **{weekday_stats.index[0]}** ({weekday_stats.iloc[0, 0]:,.0f} ƒë√°nh gi√°).
    - Ng√†y c√≥ ƒëi·ªÉm trung b√¨nh cao nh·∫•t: **{weekday_stats['ƒêi·ªÉm trung b√¨nh'].idxmax()}** ({weekday_stats['ƒêi·ªÉm trung b√¨nh'].max():.2f} sao).
    """)

elif choice == "Modeling - Evaluation":
    # Title
    st.markdown("""
    <h1 style="text-align: center; color: #4CAF50;">Modeling - Evaluation</h1>
    """, unsafe_allow_html=True)
    
    # Introduction
    st.markdown("""
    <h2 style="color: #FF4500;">T·ªïng quan</h2>
    <p>Sau khi x·ª≠ l√Ω <b>TF-IDF</b> v·ªõi feature <code>new_content</code>, th·ª±c hi·ªán ph√¢n t√≠ch v√† d·ª± ƒëo√°n v·ªõi c√°c output:</p>
    <ul>
        <li><b>positive</b></li>
        <li><b>negative</b></li>
        <li><b>neutral</b></li>
    </ul>
    <p>X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu b·∫±ng ph∆∞∆°ng ph√°p <b>OverSampling</b> ƒë·ªÉ c√¢n b·∫±ng gi·ªØa c√°c nh√£n ƒë·∫ßu ra.</p>
    """, unsafe_allow_html=True)

    # Section: Machine Learning Models
    st.markdown("""
    <h2 style="color: #4CAF50;">C√°c model Machine Learning ƒë∆∞·ª£c s·ª≠ d·ª•ng:</h2>
    """, unsafe_allow_html=True)
    models = [
        "Logistic Regression",
        "Gaussian Naive Bayes",
        "K-Nearest Neighbors",
        "Decision Tree",
        "Random Forest",
        "Extra Trees",
        "AdaBoost",
        "XGBoost",
        "Support Vector Machine (Linear Kernel)",
        "Support Vector Machine (Polynomial Kernel)"
    ]
    st.write(", ".join(models))
    
    # Section: Performance Comparison Table
    st.markdown("""
    <h2 style="color: #FF4500;">B·∫£ng so s√°nh hi·ªáu su·∫•t gi·ªØa c√°c m√¥ h√¨nh</h2>
    """, unsafe_allow_html=True)
    compare_path = "image/model_compare.png"  # Update to the correct path if necessary
    st.image(compare_path, caption="B·∫£ng so s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh", use_container_width=True)

    # Section: Insights and Model Selection
    st.markdown("""
    <h2 style="color: #4CAF50;">Nh·∫≠n x√©t v√† ch·ªçn m√¥ h√¨nh</h2>
    """, unsafe_allow_html=True)
    st.markdown("""
    - D·ª±a v√†o b·∫£ng so s√°nh, m√¥ h√¨nh <b>ExtraTreesClassifier</b> ƒë∆∞·ª£c ch·ªçn v√¨:
        - ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test (<code>accuracy_test</code>) v√† t·∫≠p train (<code>accuracy_train</code>) ƒë·ªÅu cao nh·∫•t.
        - S·ª± ch√™nh l·ªách gi·ªØa ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p train v√† test kh√¥ng qu√° l·ªõn (<b>‚âà0.45%</b>).
        - Th·ªùi gian ch·∫°y nhanh (<b>36s</b>), ph√π h·ª£p cho ·ª©ng d·ª•ng th·ª±c t·∫ø.
    """, unsafe_allow_html=True)

    # Section: Confusion Matrix
    st.markdown("""
    <h2 style="color: #FF4500;">Confusion Matrix</h2>
    """, unsafe_allow_html=True)
    confusion_path = "image/confusion_matrix.png"  # Update to the correct path if necessary
    st.image(confusion_path, use_container_width=True)

    # ROC and Precision-Recall Curves
    st.markdown("""
    <h2 style="color: #4CAF50;">ROC-AUC and Precision-Recall Curves</h2>
    """, unsafe_allow_html=True)
    roc_path = "image/roc.png"  # Update to the correct path
    st.image(roc_path, caption="ROC-AUC and Precision-Recall Curves", use_container_width=True)

    # Insights about the curves
    st.markdown("""
    <h2 style="color: #FF4500;">Nh·∫≠n x√©t:</h2>
    <ul>
        <li><b>ROC-AUC Curve:</b>
            <ul>
                <li>M√¥ h√¨nh ƒë·∫°t ƒë∆∞·ª£c AUC cao (g·∫ßn <b>0.97-0.98</b>) ·ªü c·∫£ ba l·ªõp, ch·ª©ng t·ªè m√¥ h√¨nh ph√¢n bi·ªát r·∫•t t·ªët gi·ªØa c√°c nh√£n.</li>
                <li>ƒê∆∞·ªùng cong n·∫±m g·∫ßn g√≥c tr√™n b√™n tr√°i, cho th·∫•y m√¥ h√¨nh ho·∫°t ƒë·ªông hi·ªáu qu·∫£ v·ªÅ vi·ªác gi·∫£m False Positives v√† tƒÉng True Positives.</li>
            </ul>
        </li>
        <li><b>Precision-Recall Curve:</b>
            <ul>
                <li>M√¥ h√¨nh ƒë·∫°t ƒë·ªô ch√≠nh x√°c cao (precision) tr√™n l·ªõp 2 (AUC = <b>1.0</b>), nh∆∞ng th·∫•p h∆°n v·ªõi l·ªõp 1 v√† l·ªõp 0.</li>
                <li>V·ªõi l·ªõp c√≥ AUC = <b>0.8</b> (Class 1), ƒë·ªô ch√≠nh x√°c gi·∫£m d·∫ßn khi t·ª∑ l·ªá h·ªìi ƒë√°p (recall) tƒÉng.</li>
                <li>ƒêi·ªÅu n√†y cho th·∫•y c·∫ßn ki·ªÉm tra th√™m c√°c l·ªõp d·ªØ li·ªáu ƒë·ªÉ gi·∫£m thi·ªÉu vi·ªác ƒë√°nh ƒë·ªïi gi·ªØa precision v√† recall.</li>
            </ul>
        </li>
    </ul>
    <h3 style="color: #FF4500;">T·ªïng quan:</h3>
    <p>- M√¥ h√¨nh <b>ExtraTreesClassifier</b> c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i m·∫°nh m·∫Ω, ƒë·∫∑c bi·ªát tr√™n c√°c nh√£n ch√≠nh x√°c cao (Class 2).</p>
    <p>- Precision-Recall Curves g·ª£i √Ω r·∫±ng c√≥ th·ªÉ c·∫£i thi·ªán precision cho c√°c l·ªõp th·∫•p h∆°n.</p>
    """, unsafe_allow_html=True)




elif choice == "Comment Sentiment Analysis":
    # Title and logo
    logo_hasaki_path = "image/hasaki_banner.jpg"  # Replace with the path to your banner/logo
    st.image(logo_hasaki_path, width=700)
    st.title("Comment Sentiment Analysis")

    # Choose between entering a single comment manually or loading from a file
    analysis_option = st.radio(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu:",
        ("Nh·∫≠p m·ªôt comment", "T·∫£i file danh s√°ch comment (.txt)")
    )

    if analysis_option == "Nh·∫≠p m·ªôt comment":
        st.subheader("Nh·∫≠p comment c·∫ßn ph√¢n t√≠ch:")
        input_text = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n c·∫ßn ph√¢n t√≠ch:")
        
        if st.button("Ph√¢n t√≠ch", key="single"):
            if input_text:
                with st.spinner("ƒêang ph√¢n t√≠ch..."):
                    result = predict_sentiment(input_text, vectorizer, model, loaded_le)
                    if 'error' not in result:
                        st.success(f"C·∫£m x√∫c: {result['sentiment']}")
                    else:
                        st.error(f"L·ªói: {result['error']}")
            else:
                st.warning("Vui l√≤ng nh·∫≠p b√¨nh lu·∫≠n c·∫ßn ph√¢n t√≠ch")

    elif analysis_option == "T·∫£i file danh s√°ch comment (.txt)":
        st.subheader("T·∫£i file ch·ª©a danh s√°ch comment (ƒë·ªãnh d·∫°ng .txt):")

        # Th√™m upload file
        uploaded_file = st.file_uploader(
            "K√©o th·∫£ ho·∫∑c ch·ªçn file text ch·ª©a b√¨nh lu·∫≠n (m·ªói d√≤ng m·ªôt b√¨nh lu·∫≠n)",
            type=['txt']
        )
        
        # Text area cho nh·∫≠p tr·ª±c ti·∫øp
        input_texts = st.text_area(
            "Nh·∫≠p danh s√°ch b√¨nh lu·∫≠n (m·ªói b√¨nh lu·∫≠n m·ªôt d√≤ng):",
            height=200
        )

        if st.button("Ph√¢n t√≠ch", key="batch"):
            texts = []
            
            # X·ª≠ l√Ω file upload n·∫øu c√≥
            if uploaded_file is not None:
                text_content = uploaded_file.getvalue().decode('utf-8')
                texts.extend([line.strip() for line in text_content.split('\n') if line.strip()])
                
            # Th√™m text t·ª´ text area n·∫øu c√≥ 
            if input_texts:
                texts.extend([text.strip() for text in input_texts.split('\n') if text.strip()])

            if texts:
                results = []
                
                progress_bar = st.progress(0)
                for i, text in enumerate(texts):
                    result = predict_sentiment(text, vectorizer, model, loaded_le)
                    if 'error' not in result:
                        results.append({
                            'text': text,
                            'sentiment': result['sentiment'],
                            **result['features']
                        })
                    progress_bar.progress((i + 1) / len(texts))
                
                if results:
                    df = pd.DataFrame(results)
                    st.success(f"ƒê√£ ph√¢n t√≠ch {len(results)} b√¨nh lu·∫≠n")
                    
                    # Th·ªëng k√™
                    st.subheader("Th·ªëng k√™")
                    col1, col2 = st.columns(2)
                    with col1:
                        sentiment_counts = df['sentiment'].value_counts()
                        st.bar_chart(sentiment_counts)
                    
                    with col2:
                        st.write("Ph√¢n b·ªë c·∫£m x√∫c:")
                        for sentiment, count in sentiment_counts.items():
                            st.write(f"{sentiment}: {count} ({count/len(df)*100:.1f}%)")
                    
                    # K·∫øt qu·∫£ chi ti·∫øt
                    st.subheader("K·∫øt qu·∫£ chi ti·∫øt")
                    st.dataframe(df)
                    
                    # T·∫£i xu·ªëng k·∫øt qu·∫£
                    csv = df.to_csv(index=False)
                    st.download_button(
                        "T·∫£i xu·ªëng k·∫øt qu·∫£ (CSV)",
                        csv,
                        "sentiment_analysis_results.csv",
                        "text/csv"
                    )
                else:
                    st.error("Kh√¥ng th·ªÉ ph√¢n t√≠ch b√¨nh lu·∫≠n n√†o")
            else:
                st.warning("Vui l√≤ng nh·∫≠p ho·∫∑c t·∫£i l√™n file ch·ª©a b√¨nh lu·∫≠n c·∫ßn ph√¢n t√≠ch")


elif choice == "Product Sentiment Analysis":
    # Title
    st.title("Product Sentiment Analysis")
    st.subheader("Ch·ªçn s·∫£n ph·∫©m ƒë·ªÉ xem ƒë√°nh gi√°")

    # Check if data is loaded
    if 'df_summary' in locals() and not df_summary.empty:
        # Randomly select 30 products
        random_products = df_summary.sample(n=30, random_state=42)[['ten_san_pham', 'ma_san_pham']]

        # Create product options as tuples of (name, ID)
        product_options = [(row['ten_san_pham'], row['ma_san_pham']) for _, row in random_products.iterrows()]

        # Selectbox for product selection
        selected_product = st.selectbox(
            "Ch·ªçn s·∫£n ph·∫©m",
            options=product_options,
            format_func=lambda x: x[0]  # Display only the product name
        )

        # Display selected product details
        st.write("### B·∫°n ƒë√£ ch·ªçn:")
        st.write(f"**T√™n s·∫£n ph·∫©m:** {selected_product[0]}")
        st.write(f"**M√£ s·∫£n ph·∫©m:** {selected_product[1]}")

        # Analysis and visualization options
        if selected_product:
            product_id = selected_product[1]  # Extract product ID
            product_data = df_summary[df_summary['ma_san_pham'] == product_id]

        if not product_data.empty:
            # Display basic product information
            product_name = product_data['ten_san_pham'].iloc[0]
            avg_rating = product_data['so_sao'].mean()
            total_reviews = len(product_data)
            unique_customers = product_data['ma_khach_hang'].nunique()

            # Count sentiment categories
            sentiment_counts = product_data['output'].value_counts()
            positive_reviews = sentiment_counts.get('positive', 0)
            neutral_reviews = sentiment_counts.get('neutral', 0)
            negative_reviews = sentiment_counts.get('negative', 0)
            # Display product information
            st.write(f"**ƒêi·ªÉm ƒë√°nh gi√° trung b√¨nh:** {avg_rating:.2f}")
            st.write(f"**T·ªïng s·ªë nh·∫≠n x√©t:** {total_reviews}")
            st.write(f"**S·ªë l∆∞·ª£ng m√£ kh√°ch h√†ng duy nh·∫•t:** {unique_customers}")
            st.write(f"**S·ªë nh·∫≠n x√©t t√≠ch c·ª±c:** {positive_reviews}")
            st.write(f"**S·ªë nh·∫≠n x√©t trung t√≠nh:** {neutral_reviews}")
            st.write(f"**S·ªë nh·∫≠n x√©t ti√™u c·ª±c:** {negative_reviews}")
            display_wordclouds(product_data)
            # Choose the filter option
            st.write("### T√πy ch·ªçn ph√¢n t√≠ch:")
            filter_option = st.selectbox("Ch·ªçn c√°ch hi·ªÉn th·ªã:", ["T·∫•t c·∫£ c√°c nƒÉm", "Theo th√°ng"])

            # Additional dropdown for year/month selection
            selected_year = None
            selected_month = None
            if filter_option == "Theo th√°ng":
                selected_year = st.selectbox("Ch·ªçn nƒÉm:", sorted(product_data['nam'].unique()))

            # Call the function to display charts
            display_analysis_charts(product_data, product_name, filter_option, selected_year)
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho s·∫£n ph·∫©m n√†y.")
    else:
        st.warning("Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ file df_summary.csv. Vui l√≤ng ki·ªÉm tra file d·ªØ li·ªáu.")



def display_team_members_in_sidebar(members):
    st.sidebar.markdown('<h2 style="color:#0047AB; text-align: left;">Th√†nh vi√™n nh√≥m:</h2>', unsafe_allow_html=True)
    member_html = '<div style="color:#0047AB; font-size: 18px; text-align: left; font-weight: bold;">' + \
                  '<br>'.join(members) + \
                  '</div>'
    st.sidebar.markdown(member_html, unsafe_allow_html=True)

team_members = ["1. Phan Minh Hu·ªá", "2. Hu·ª≥nh Danh Nh√¢n"]
display_team_members_in_sidebar(team_members)